{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "flickr_data = json.load(open('eval_dataset/Flickr_Dataset.json'))\n",
    "all_data = json.load(open('eval_dataset/FlickrCOCO_Dataset.json'))\n",
    "data = flickr_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_within_span(token_positives, span):\n",
    "    assert len(span) == 2\n",
    "    start_idx, end_idx = span[0], span[1]\n",
    "    for span in token_positives:\n",
    "        if start_idx >= span[0] and end_idx <= span[1]:\n",
    "            return True\n",
    "def find_dp_word_len(dpid):\n",
    "    return len(data['grounding_pairs'][dpid]['data']['img_info']['caption'])\n",
    "def find_phrase_num(dpid):\n",
    "    return len(data['grounding_pairs'][dpid]['data']['noun_phrase_groups'])\n",
    "def find_bbox_num(dpid):\n",
    "    return len(data['grounding_pairs'][dpid]['data']['annotations'])\n",
    "def find_bboxs_coverage_for_dpid_word(dpid, word):\n",
    "    covs = []\n",
    "    this_data = data['grounding_pairs'][dpid]['data']\n",
    "    img_size = int(this_data['img_info']['width']) * int(this_data['img_info']['height'])\n",
    "    word_span = None\n",
    "    for group in this_data['noun_phrase_groups']:\n",
    "        has = False\n",
    "        for reg in group['region']:\n",
    "            if reg[1] == word:\n",
    "                has = True\n",
    "                word_span = reg[3]\n",
    "                break\n",
    "        if has:\n",
    "            for ann in this_data['annotations']:\n",
    "                # this trivial implementation is buggy\n",
    "                # if ann['bbox_info']['tokens_positive'] == spans:\n",
    "                if is_within_span(ann['bbox_info']['tokens_positive'], word_span):\n",
    "                    box = ann['bbox_info']['bbox']\n",
    "                    b_s = box[-1] * box[-2]\n",
    "                    cov = b_s / img_size\n",
    "                    covs.append(cov)\n",
    "    return covs\n",
    "def find_bboxs_for_dpid_word(dpid, word):\n",
    "    bboxs = []\n",
    "    this_data = data['grounding_pairs'][dpid]['data']\n",
    "    word_span = None\n",
    "    for group in this_data['noun_phrase_groups']:\n",
    "        has = False\n",
    "        for reg in group['region']:\n",
    "            if reg[1] == word:\n",
    "                has = True\n",
    "                word_span = reg[3]\n",
    "                break\n",
    "        if has:\n",
    "            for ann in this_data['annotations']:\n",
    "                if is_within_span(ann['bbox_info']['tokens_positive'], word_span):\n",
    "                    bboxs.append(ann['bbox_info']['bbox'])\n",
    "    return bboxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = set()\n",
    "for t in data['tests']:\n",
    "    word_list.add(t['mask_regions'][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "word_dpid_map = defaultdict(list)\n",
    "\n",
    "for dpid, t in enumerate(data['tests']):\n",
    "    word_dpid_map[t['mask_regions'][0][1]].append(t['pair_id'])\n",
    "def find_corresponded_dpids_for_words(K):\n",
    "    return word_dpid_map[K]\n",
    "def mean(l):\n",
    "    return sum(l) / len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4214/4214 [00:00<00:00, 11243.31it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "test_word_feats = {}\n",
    "\n",
    "for word in tqdm(word_list):\n",
    "    dps =find_corresponded_dpids_for_words(word)\n",
    "    phrase_nums = [find_phrase_num(i) for i in dps]\n",
    "    box_nums = [find_bbox_num(i) for i in dps]\n",
    "    bbox_covs = [mean(find_bboxs_coverage_for_dpid_word(i, word)) for i in dps]\n",
    "    test_word_feats[word] = {'word_freq':len(dps), 'avg_cooccur_phrase':mean(phrase_nums), 'avg_cooccur_box': mean(box_nums), 'avg_bbox_cov': mean(bbox_covs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"all-test-word-feats-fixed.json\", 'w') as f:\n",
    "    json.dump(test_word_feats, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit ('na')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "45fc85d84af5a2ebbe3e64508f6517ce96c0a617a252a04a92ee7a9ae799aad7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
